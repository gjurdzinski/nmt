[2018-01-17 21:42:30] Created model with following parameters:
        name: nmt
        cell: LSTM
        num_units: 512
        num_layers: 4
        embeddings_size: 512
        dropout: 0.2
        optimizer: gd
        learning_rate: 1.0
        decay_factor: 0.8
        start_decay_step: 15000
        decay_steps: 2000
        batch_size: 32
        infer_batch_size: 16
        bidirectional_encoder: True
        bi_reduce: layers
        attention: luong
        infer_helper: greedy
        time_major: False
        max_gradient_norm: 5.0
        src_max_len: 85
        tgt_max_len: 85
        checkpoints_path: ./check_attention
        src_filename: /home/z1137405/Rozne/Datasets/vi/train2.vi
        tgt_filename: /home/z1137405/Rozne/Datasets/vi/train2.en
        src_vocab_filename: /home/z1137405/Rozne/Datasets/vi/vocab2.vi
        tgt_vocab_filename: /home/z1137405/Rozne/Datasets/vi/vocab2.en
        src_vocab_size: 24350
        tgt_vocab_size: 30000
        test_src_filename: /home/z1137405/Rozne/Datasets/vi/tst20132.vi
        test_tgt_filename: /home/z1137405/Rozne/Datasets/vi/tst20132.en
[2018-01-17 21:42:30] Starting training
E 1/12: 100%|███████████████████████████████████████████████████████████████| 4161/4161 [1:27:14<00:00,  1.26s/it, b2=1.5, bleu=1.46, l=97.2, lrate=1, p=92.3]
[2018-01-17 23:11:08] After epoch 1/12, bleu: 1.2354, b2: 1.2586, avg loss: 120.9623
E 2/12: 100%|████████████████████████████████████████████████████████████████| 4161/4161 [1:10:10<00:00,  1.01s/it, b2=11, bleu=10.9, l=70.7, lrate=1, p=26.9]
[2018-01-18 00:22:20] After epoch 2/12, bleu: 10.0359, b2: 10.1223, avg loss: 80.3623
E 3/12: 100%|████████████████████████████████████████████████████████████████| 4161/4161 [1:13:14<00:00,  1.06s/it, b2=14, bleu=13.8, l=61.2, lrate=1, p=17.3]
[2018-01-18 01:36:22] After epoch 3/12, bleu: 14.2786, b2: 14.4202, avg loss: 64.9106
E 4/12: 100%|████████████████████████████████████████████████████████████████| 4161/4161 [1:08:55<00:00,  1.01it/s, b2=16.3, bleu=16.1, l=55.1, lrate=1, p=13]
[2018-01-18 02:46:05] After epoch 4/12, bleu: 15.5865, b2: 15.7069, avg loss: 57.6356
E 5/12: 100%|███████████████████████████████████████████████████████████| 4161/4161 [1:13:45<00:00,  1.06s/it, b2=19.3, bleu=19.1, l=49.8, lrate=0.64, p=10.2]
[2018-01-18 04:00:36] After epoch 5/12, bleu: 19.2222, b2: 19.3607, avg loss: 51.1662
E 6/12: 100%|███████████████████████████████████████████████████████████████| 4161/4161 [1:13:31<00:00,  1.06s/it, b2=21, bleu=20.8, l=45, lrate=0.41, p=8.15]
[2018-01-18 05:14:49] After epoch 6/12, bleu: 21.5003, b2: 21.6540, avg loss: 45.7051
E 7/12: 100%|███████████████████████████████████████████████████████████| 4161/4161 [1:10:28<00:00,  1.02s/it, b2=21.9, bleu=21.7, l=40.9, lrate=0.21, p=6.71]
[2018-01-18 06:26:10] After epoch 7/12, bleu: 22.2869, b2: 22.4161, avg loss: 42.0660
E 8/12: 100%|██████████████████████████████████████████████████████████| 4161/4161 [1:12:30<00:00,  1.05s/it, b2=22.5, bleu=22.3, l=39.1, lrate=0.134, p=6.19]
[2018-01-18 07:39:20] After epoch 8/12, bleu: 22.6419, b2: 22.8198, avg loss: 39.6271
E 9/12: 100%|█████████████████████████████████████████████████████████| 4161/4161 [1:08:55<00:00,  1.01it/s, b2=23.3, bleu=23.2, l=37.4, lrate=0.0859, p=5.71]
[2018-01-18 08:48:56] After epoch 9/12, bleu: 23.2413, b2: 23.4057, avg loss: 37.9958
E 10/12: 100%|█████████████████████████████████████████████████████████| 4161/4161 [1:08:53<00:00,  1.01it/s, b2=23.5, bleu=23.4, l=36.9, lrate=0.055, p=5.59]
[2018-01-18 09:58:30] After epoch 10/12, bleu: 23.6064, b2: 23.7266, avg loss: 36.9315
E 11/12: 100%|████████████████████████████████████████████████████████| 4161/4161 [1:08:58<00:00,  1.01it/s, b2=23.5, bleu=23.4, l=36.8, lrate=0.0352, p=5.56]
[2018-01-18 11:08:09] After epoch 11/12, bleu: 23.5319, b2: 23.6698, avg loss: 36.2319
E 12/12: 100%|████████████████████████████████████████████████████████| 4161/4161 [1:08:24<00:00,  1.01it/s, b2=23.8, bleu=23.6, l=36.9, lrate=0.0225, p=5.57]
[2018-01-18 12:17:23] After epoch 12/12, bleu: 23.6478, b2: 23.7838, avg loss: 35.7735
